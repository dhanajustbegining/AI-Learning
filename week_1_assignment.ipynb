{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDtYk2N+A+dnWMF2YoHfpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanajustbegining/AI-Learning/blob/main/week_1_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2I64P5cEEgo",
        "outputId": "8eaf4605-9eab-46f9-9567-69eee96df661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Vrddhi\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello Vrddhi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright — let’s go step-by-step and calculate each metric carefully.\n",
        "\n",
        "We’re given:\n",
        "\n",
        "* **TP (True Positives)** = 80\n",
        "* **TN (True Negatives)** = 50\n",
        "* **FP (False Positives)** = 20\n",
        "* **FN (False Negatives)** = 10\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Precision**\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Substitute:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.8\n",
        "$$\n",
        "\n",
        "✅ **Precision = 0.8 (80%)**\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Recall**\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Substitute:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{80}{80 + 10} = \\frac{80}{90} \\approx 0.8889\n",
        "$$\n",
        "\n",
        "✅ **Recall ≈ 0.8889 (88.89%)**\n",
        "\n",
        "---\n",
        "\n",
        "## **3. F1-score**\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "Substitute:\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{0.8 \\times 0.8889}{0.8 + 0.8889}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{0.71112}{1.6889} \\approx 2 \\times 0.4211 \\approx 0.842\n",
        "$$\n",
        "\n",
        "✅ **F1-score ≈ 0.842 (84.2%)**\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Accuracy**\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "Substitute:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{80 + 50}{80 + 50 + 20 + 10} = \\frac{130}{160} = 0.8125\n",
        "$$\n",
        "\n",
        "✅ **Accuracy = 0.8125 (81.25%)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Answers**\n",
        "\n",
        "* **Precision:** 0.8 (80%)\n",
        "* **Recall:** ≈ 0.8889 (88.89%)\n",
        "* **F1-score:** ≈ 0.842 (84.2%)\n",
        "* **Accuracy:** 0.8125 (81.25%)\n",
        "\n",
        "---\n",
        "\n",
        "If you’d like, I can also give you a **Python snippet** so you can calculate these for any TP, TN, FP, FN values without doing them manually.\n",
        "Do you want me to prepare that?\n"
      ],
      "metadata": {
        "id": "oZkdP5JkT_M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate metrics\n",
        "def calculate_metrics(TP, TN, FP, FN):\n",
        "    # Precision\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    # Recall\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "    # F1-score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "    return precision, recall, f1_score, accuracy\n",
        "\n",
        "\n",
        "# Given values\n",
        "TP = 80\n",
        "TN = 50\n",
        "FP = 20\n",
        "FN = 10\n",
        "\n",
        "# Calculate metrics\n",
        "precision, recall, f1, accuracy = calculate_metrics(TP, TN, FP, FN)\n",
        "\n",
        "# Display results\n",
        "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "print(f\"F1-score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
        "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnAN32JeTEOx",
        "outputId": "3ac7af8a-44c1-46d6-d903-a3fa3d3e1c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8000 (80.00%)\n",
            "Recall:    0.8889 (88.89%)\n",
            "F1-score:  0.8421 (84.21%)\n",
            "Accuracy:  0.8125 (81.25%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright — let’s calculate for **both models** step-by-step so we don’t make mistakes.\n",
        "\n",
        "---\n",
        "\n",
        "## **Given Data**\n",
        "\n",
        "**Model A**\n",
        "\n",
        "* TP = 120\n",
        "* TN = 150\n",
        "* FP = 30\n",
        "* FN = 20\n",
        "\n",
        "**Model B**\n",
        "\n",
        "* TP = 140\n",
        "* TN = 130\n",
        "* FP = 40\n",
        "* FN = 10\n",
        "\n",
        "---\n",
        "\n",
        "## **Formulas**\n",
        "\n",
        "* **Precision** = TP / (TP + FP)\n",
        "* **Recall** = TP / (TP + FN)\n",
        "* **F1-score** = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model A**\n",
        "\n",
        "**Precision:**\n",
        "\n",
        "$$\n",
        "\\text{Precision}_A = \\frac{120}{120 + 30} = \\frac{120}{150} = 0.8\n",
        "$$\n",
        "\n",
        "**Recall:**\n",
        "\n",
        "$$\n",
        "\\text{Recall}_A = \\frac{120}{120 + 20} = \\frac{120}{140} \\approx 0.8571\n",
        "$$\n",
        "\n",
        "**F1-score:**\n",
        "\n",
        "$$\n",
        "\\text{F1}_A = 2 \\times \\frac{0.8 \\times 0.8571}{0.8 + 0.8571}\n",
        "= 2 \\times \\frac{0.68568}{1.6571} \\approx 0.8276\n",
        "$$\n",
        "\n",
        "✅ **Model A**\n",
        "\n",
        "* Precision = 0.8000 (80%)\n",
        "* Recall ≈ 0.8571 (85.71%)\n",
        "* F1-score ≈ 0.8276 (82.76%)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Model B**\n",
        "\n",
        "**Precision:**\n",
        "\n",
        "$$\n",
        "\\text{Precision}_B = \\frac{140}{140 + 40} = \\frac{140}{180} \\approx 0.7778\n",
        "$$\n",
        "\n",
        "**Recall:**\n",
        "\n",
        "$$\n",
        "\\text{Recall}_B = \\frac{140}{140 + 10} = \\frac{140}{150} \\approx 0.9333\n",
        "$$\n",
        "\n",
        "**F1-score:**\n",
        "\n",
        "$$\n",
        "\\text{F1}_B = 2 \\times \\frac{0.7778 \\times 0.9333}{0.7778 + 0.9333}\n",
        "= 2 \\times \\frac{0.72593}{1.7111} \\approx 0.8485\n",
        "$$\n",
        "\n",
        "✅ **Model B**\n",
        "\n",
        "* Precision ≈ 0.7778 (77.78%)\n",
        "* Recall ≈ 0.9333 (93.33%)\n",
        "* F1-score ≈ 0.8485 (84.85%)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Comparison**\n",
        "\n",
        "| Metric    | Model A | Model B | Better Model |\n",
        "| --------- | ------- | ------- | ------------ |\n",
        "| Precision | 0.8000  | 0.7778  | **A**        |\n",
        "| Recall    | 0.8571  | 0.9333  | **B**        |\n",
        "| F1-score  | 0.8276  | 0.8485  | **B**        |\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Summary:**\n",
        "\n",
        "* **Precision:** Model A is better (fewer false positives).\n",
        "* **Recall:** Model B is better (fewer false negatives).\n",
        "* **F1-score:** Model B is slightly better overall.\n",
        "\n",
        "---\n",
        "\n",
        "If you like, I can give you a **Python function** to compare *any* two models’ precision, recall, and F1-score instantly. That way you won’t need to calculate manually again.\n",
        "Do you want me to make that?\n"
      ],
      "metadata": {
        "id": "yLZ9URoyToiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(TP, TN, FP, FN):\n",
        "    \"\"\"Calculate precision, recall, and F1-score.\"\"\"\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "def compare_models(modelA, modelB):\n",
        "    # Unpack model data\n",
        "    TP_A, TN_A, FP_A, FN_A = modelA\n",
        "    TP_B, TN_B, FP_B, FN_B = modelB\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision_A, recall_A, f1_A = calculate_metrics(TP_A, TN_A, FP_A, FN_A)\n",
        "    precision_B, recall_B, f1_B = calculate_metrics(TP_B, TN_B, FP_B, FN_B)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Model A:\")\n",
        "    print(f\" Precision: {precision_A:.4f} ({precision_A*100:.2f}%)\")\n",
        "    print(f\" Recall:    {recall_A:.4f} ({recall_A*100:.2f}%)\")\n",
        "    print(f\" F1-score:  {f1_A:.4f} ({f1_A*100:.2f}%)\\n\")\n",
        "\n",
        "    print(\"Model B:\")\n",
        "    print(f\" Precision: {precision_B:.4f} ({precision_B*100:.2f}%)\")\n",
        "    print(f\" Recall:    {recall_B:.4f} ({recall_B*100:.2f}%)\")\n",
        "    print(f\" F1-score:  {f1_B:.4f} ({f1_B*100:.2f}%)\\n\")\n",
        "\n",
        "    # Comparison\n",
        "    print(\"Better Model by Metric:\")\n",
        "    print(f\" Precision: {'Model A' if precision_A > precision_B else 'Model B'}\")\n",
        "    print(f\" Recall:    {'Model A' if recall_A > recall_B else 'Model B'}\")\n",
        "    print(f\" F1-score:  {'Model A' if f1_A > f1_B else 'Model B'}\")\n",
        "\n",
        "\n",
        "# Example usage with your data\n",
        "modelA_data = (120, 150, 30, 20)  # TP, TN, FP, FN\n",
        "modelB_data = (140, 130, 40, 10)\n",
        "\n",
        "compare_models(modelA_data, modelB_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0ouW94gTqAh",
        "outputId": "14b8156e-2034-4115-bc91-335b2d0327f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model A:\n",
            " Precision: 0.8000 (80.00%)\n",
            " Recall:    0.8571 (85.71%)\n",
            " F1-score:  0.8276 (82.76%)\n",
            "\n",
            "Model B:\n",
            " Precision: 0.7778 (77.78%)\n",
            " Recall:    0.9333 (93.33%)\n",
            " F1-score:  0.8485 (84.85%)\n",
            "\n",
            "Better Model by Metric:\n",
            " Precision: Model A\n",
            " Recall:    Model B\n",
            " F1-score:  Model B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, let’s work through this step-by-step so we get everything correct.\n",
        "\n",
        "---\n",
        "\n",
        "## **Given Data**\n",
        "\n",
        "* Total images = 200\n",
        "* TP = 70\n",
        "* TN = 100\n",
        "* FP = 10\n",
        "* FN = 20\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Accuracy**\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{\\text{Total samples}}\n",
        "$$\n",
        "\n",
        "**Calculation:**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{70 + 100}{200} = \\frac{170}{200} = 0.85\n",
        "$$\n",
        "\n",
        "✅ **Accuracy = 0.85 (85%)**\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Precision vs. Recall**\n",
        "\n",
        "* **Precision** = TP / (TP + FP)\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{70}{70 + 10} = \\frac{70}{80} = 0.875 \\ (87.5\\%)\n",
        "$$\n",
        "\n",
        "* **Recall** = TP / (TP + FN)\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{70}{70 + 20} = \\frac{70}{90} \\approx 0.7778 \\ (77.78\\%)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation**\n",
        "\n",
        "* **Precision (87.5%)** is higher than **Recall (77.78%)**.\n",
        "* Higher precision means **fewer false positives** — the car rarely mistakes a non-pedestrian for a pedestrian.\n",
        "* Lower recall means **more false negatives** — the car misses some actual pedestrians.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion:**\n",
        "The model is **better at avoiding false positives** than avoiding false negatives.\n",
        "However, in a pedestrian detection scenario, missing pedestrians (false negatives) could be more dangerous than a few extra false positives.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can show you **how to visualize this with a confusion matrix** so the trade-off between false positives and false negatives is easier to see. Would you like me to prepare that?\n"
      ],
      "metadata": {
        "id": "3yFX7tcCUawg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(TP, TN, FP, FN, total_samples):\n",
        "    # Calculate metrics\n",
        "    accuracy = (TP + TN) / total_samples\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "\n",
        "    # Interpretation\n",
        "    if precision > recall:\n",
        "        print(\"\\nModel is better at avoiding False Positives.\")\n",
        "    elif recall > precision:\n",
        "        print(\"\\nModel is better at avoiding False Negatives.\")\n",
        "    else:\n",
        "        print(\"\\nModel performs equally for avoiding False Positives and False Negatives.\")\n",
        "\n",
        "# Example usage with your data\n",
        "evaluate_model(TP=70, TN=100, FP=10, FN=20, total_samples=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5H4_VcUjNu",
        "outputId": "39c309c6-6c43-45d7-fccb-768549be0951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8500 (85.00%)\n",
            "Precision: 0.8750 (87.50%)\n",
            "Recall:    0.7778 (77.78%)\n",
            "\n",
            "Model is better at avoiding False Positives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s go step-by-step so the math is clear.\n",
        "\n",
        "---\n",
        "\n",
        "## **Given Data**\n",
        "\n",
        "* Total samples = 1000\n",
        "* TP = 400\n",
        "* TN = 450\n",
        "* FP = 50\n",
        "* FN = 100\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Metric Calculations**\n",
        "\n",
        "#### **Precision**\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{400}{400 + 50} = \\frac{400}{450} \\approx 0.8889 \\ (88.89\\%)\n",
        "$$\n",
        "\n",
        "#### **Recall**\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{400}{400 + 100} = \\frac{400}{500} = 0.8 \\ (80\\%)\n",
        "$$\n",
        "\n",
        "#### **F1-score**\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{0.8889 \\times 0.8}{0.8889 + 0.8} = 2 \\times \\frac{0.7111}{1.6889} \\approx 0.8421 \\ (84.21\\%)\n",
        "$$\n",
        "\n",
        "#### **Accuracy**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{\\text{Total Samples}} = \\frac{400 + 450}{1000} = \\frac{850}{1000} = 0.85 \\ (85\\%)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Summary Table**\n",
        "\n",
        "| Metric    | Value  |\n",
        "| --------- | ------ |\n",
        "| Precision | 88.89% |\n",
        "| Recall    | 80.00% |\n",
        "| F1-score  | 84.21% |\n",
        "| Accuracy  | 85.00% |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Imbalanced Dataset Impact**\n",
        "\n",
        "If the dataset were **imbalanced** — for example, **80% negative** and **20% positive**:\n",
        "\n",
        "* Accuracy could still be **high** even if the model performs poorly for the minority (positive) class.\n",
        "* Example: If the model simply predicts **\"negative\" for everything**:\n",
        "\n",
        "  * Accuracy = 80% (because it gets all negatives correct)\n",
        "  * But **Precision, Recall, and F1-score** for the positive class would be **0**, showing no real predictive power for positives.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "* **Accuracy is not reliable** in imbalanced datasets.\n",
        "* Metrics like **Precision, Recall, F1-score, ROC-AUC** are better for evaluating performance in such cases.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **write a Python function** that calculates all these metrics and also simulates the effect of imbalance on accuracy.\n",
        "Do you want me to prepare that?\n"
      ],
      "metadata": {
        "id": "dRHOSsTAVWMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_binary_classifier(TP, TN, FP, FN, total_samples, imbalance_ratio=None):\n",
        "    # Metrics\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    accuracy = (TP + TN) / total_samples\n",
        "\n",
        "    print(\"=== Metrics ===\")\n",
        "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"F1-score:  {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # Simulate imbalance\n",
        "    if imbalance_ratio:\n",
        "        neg_ratio, pos_ratio = imbalance_ratio\n",
        "        new_total = total_samples\n",
        "        new_negatives = int(new_total * neg_ratio)\n",
        "        new_positives = int(new_total * pos_ratio)\n",
        "\n",
        "        # Worst-case: model predicts all negatives\n",
        "        worst_case_accuracy = new_negatives / new_total\n",
        "\n",
        "        print(\"\\n=== Imbalance Simulation ===\")\n",
        "        print(f\"Negatives: {new_negatives} ({neg_ratio*100:.1f}%), Positives: {new_positives} ({pos_ratio*100:.1f}%)\")\n",
        "        print(f\"Worst-case Accuracy (predict all negatives): {worst_case_accuracy:.4f} ({worst_case_accuracy*100:.2f}%)\")\n",
        "        print(\"This shows why accuracy alone can be misleading for imbalanced datasets.\")\n",
        "\n",
        "# Example usage with your balanced data\n",
        "evaluate_binary_classifier(TP=400, TN=450, FP=50, FN=100, total_samples=1000, imbalance_ratio=(0.8, 0.2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg5yNF9eVXrD",
        "outputId": "be998879-d864-437d-81c6-a121fa49b9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Metrics ===\n",
            "Precision: 0.8889 (88.89%)\n",
            "Recall:    0.8000 (80.00%)\n",
            "F1-score:  0.8421 (84.21%)\n",
            "Accuracy:  0.8500 (85.00%)\n",
            "\n",
            "=== Imbalance Simulation ===\n",
            "Negatives: 800 (80.0%), Positives: 200 (20.0%)\n",
            "Worst-case Accuracy (predict all negatives): 0.8000 (80.00%)\n",
            "This shows why accuracy alone can be misleading for imbalanced datasets.\n"
          ]
        }
      ]
    }
  ]
}